- 什麼是SVM？

Support vector machine(通稱SVM)是一種分類、回歸的方法，一般我們是用在機器學習(Machine learning)！是一種監督式學習的方法，可以同時運用於線性及非線性的演算法，在實際的應用方面有數字辨識、人臉辨識...等等很多不同的地方。

既然上面提到了監督式學習～就順便說一下監督式和非監督式兩者的差別吧！

>**監督式學習Supervised** <br/>
>>在放入樣本給機器做學習的時候會一併告訴機器當次的輸出的結果應該為何！例：一台機器我們需要她幫我們分類進來的水果是蘋果還是橘子，帶給訓練樣本的時候，我們會跟他說這一次進去的水果應該要幫我們分到哪一類。 

>**非監督式學習Non-Supervised** <br/>
>>在放入樣本給機器做學習的時候不提供當次的結果，由機器幫我們自動分類！例：一台機器我們要請它幫我們分類，但是給訓練樣本的時候不告訴它當次的輸出結果應該為何，而是跟機器說我們要分幾類，機器會依照特徵來將樣本分成我們要的類別數量。 


來說明一下SVM到底是怎麼做到分類的好了，基本上SVM的概念很簡單，就是在一個平面上畫上一條線來分左右邊，來達到將輸入的資料數據做分類(如圖1)，而SVM的重點就是要找到那條線！而SVM有趣的地方就是有很多條線都可以將我們的資料分得好好的(如圖2)，我們要怎麼知道哪一條線是我們的SVM要的呢?<br />
(補圖1，在座標圖上畫好分類線)
(補圖2，在座標圖上畫上多條分類線)


也就是希望我們SVM的線能幫我們分類的越準確越好，而如何說是越準確越好呢？一般來說輸入值都會有些微的誤差(如圖3)，而我們希望輸入數據如果有誤差的話他還是能夠幫我們歸類在對的一邊，也就是說我們這條線能夠離與他最近的數據之間的間距越遠越好，而在這些數據上面都會有一條與我們要求的SVM的線平行的線，而SVM的線與這些線之間的間距我們稱它為“Margin”(如圖4)。<br />
(補圖3，在座標圖上以點為圓心畫上同心圓來表示誤差範圍)
(補圖4，座標圖將我們的Margin標示起來)

所以我們的SVM所要求的這條線的條件就是他有越大的間距越好，所以當我們有多種可以選擇時我們會選擇有最大Margin的線(如圖5)，而這條線的Margin是由離他最近的單個或是多個數據點所限制出來的(如圖6)，而這樣的數據點我們也有專屬於它的名稱為“Support vector”，這也就是為什麼我們這個叫做SVM的原因了。<br />
(補圖5，有兩張座標圖且有不同的間距)
(補圖6，座標圖上將Support vector標示起來)

- SVM能夠做什麼？

那SVM能夠用在什麼樣的地方呢？其實在前面有稍微介紹一下他的應用有哪方面！而在這邊我就帶大家來稍微看一下實際SVM做出來會有什麼樣的效果，這支程式是在手機上面跑SVM來做分類！
(補程式)

- SVM的公式及算法

我們在前面介紹SVM的時候有說到我們要求的是那條最佳的分割線，然而要怎麼求呢？我們先來說一下那條線是要用什麼樣的公式表示好了。我們中間那條線的公式為
>𝒇(𝒙) ＝ 𝑾 ᵀ𝑿- 𝒃 ＝ 0 (註1)

首先我們先說一下，其實我們的公式𝒇(𝒙) ＝ 𝑾 ᵀ𝑿就夠了，但為什麼會多出一個𝒃呢？其實𝒃是我們SVM線的偏權值(Bais)，因為我們的線不一定會通過圓心(0,0)的位置，所以會有一個偏權值來幫助他做誤差的部分，然而這部分根神經網路的神經元有點像，每個神經元都會有自己的偏權值，這部分我們有機會再提吧。<br />
對了～我們還有另外也很重要的兩條線！哪兩條呢？也就是要幫助我們找出最大“Margin”的那兩條線！分別為<br />
>𝑾 ᵀ𝑿- 𝒃 ＝ 1<br />
>𝑾 ᵀ𝑿- 𝒃 ＝ -1

上面這樣講是不是有點抽象呢？那我們來看一下在圖上是如何表示(如圖7)。
對了～上述公式中有一個T的意思是表示”轉置矩陣”，而在公式中的大寫字母一般都代表”矩陣”的意思。<br />
(補圖7，座標圖上將三條線補上公式並且以紅色標示中間的線)

然後我們之後將數值丟入我們的𝒇(𝒙)時，出來的數值可能會大於1 or -1 ，當我們的數值被分類到正確的地方時要符合𝒚n (𝑾 ᵀ𝑿n- 𝒃) > 0 的部分，而𝒚n是我們在給定訓練樣本時預期他輸出的結果(1 or -1)，因為我們輸出的結果和預期的結果要為同號才代表我們的分類是正確的。

現在我們知道SVM是要找最大的Margin，也有三條線的方程式了，所以我們可以知道以下幾點：

- 我們要最大的間距距離：1 / || 𝑾 ||²  —但因為我們有兩倍的間距，所以是 —>  2 / || 𝑾 ||²
- 要符合：𝒚n (𝑾 ᵀ𝑿n- 𝒃) > 0

但因為找極大值會比較麻煩！所以我們要把1.的問題轉化成找極小值，要如何轉化呢？其實很簡單～就是把分子分母上下倒過來就可以了，所以我們的1.就會變成要求這個  || 𝑾 ||² / 2的極小值。

我們發現我們有方程式及限制式，而且又要求極值～這根本是天時地利人和！我們就可以用拉格朗日乘子(Lagrange Multiplier)來求我們的極值，我們先來講解一下使用Lagrange後會有什麼變化吧，至於什麼是Lagrange Multiplier我們之後會介紹到。
<br />
$$
\min _{ w,b }{ \max _{ a\ge 0 }{ \left( \frac { \left\| W \right\| ^{ 2 } }{ 2 } -\sum _{ i=1 }^{ n }{ { a }_{ i }\left[ { y }_{ i }\left( W\ast { X }_{ i }-b \right) -1 \right]  }  \right)  }  }$$
我們可以利用Lagrange Multiplier的特性，對𝑾及𝒃做偏微分等於零來找極值：<br />
$$\frac { \partial L }{ \partial x } =W-\sum _{ i=1 }^{ n }{ { a }_{ i }{ y }_{ i }{ x }_{ i }=0 } $$
<br />
$$\frac { \partial L }{ \partial b } =\sum _{ i=1 }^{ n }{ { a }_{ i }{ y }_{ i }=0 } $$
<br />
<br />由上面的兩個式可以知道<br />
$$W=\sum _{ i=1 }^{ n }{ { a }_{ i }{ y }_{ i }{ x }_{ i } } $$
<br />
$$\sum _{ i=1 }^{ n }{ { a }_{ i }{ y }_{ i }=0 } $$
<br />
將Lagrange Multiplier中的未知數𝑾用𝒂來替換掉，就會變成如下：
<br />
$$sum _{ i=1 }^{ n }{ { a }_{ i }-\frac { 1 }{ 2 } \sum _{ i,j=1 }^{ n }{ { a }_{ i }{ a }_{ j }{ y }_{ i }{ y }_{ j }{ { x }_{ i } }^{ T }{ x }_{ j } }  } $$
<br />
在這邊我們還用不到上面倒出來的結果，不過還是在這邊一併推導，會在之後的SMO章節中用到，我們在下面詳解SVM的部分會帶基礎的題目來讓大家看看。

- SVM詳解

這邊我們就來講講為什麼我們要找的最大的Marge是這樣的公式好了，首先我們來確認大家都知道我們的𝑾 ᵀ𝑿- 𝒃這個當𝑾 、𝑿為多維坐標(矩陣)時，就代表此方程式的圖為超平面，再來是𝑾 為該超平面的法向量，也就是垂直平面於的向量。

確認完大家都知道後，來看一下超平面𝑾 ᵀ𝑿- 𝒃=0(如圖8)，該平面上面有兩個點，分別為Ẋ和Ẍ，而這兩點都符合超平面𝑾 ᵀ𝑿- 𝒃＝0，因此帶入後我們知道
<br />
> 𝑾 ᵀẊ ＝ 𝒃
<br />
> 𝑾 ᵀẌ ＝ 𝒃
<br />

我們把這兩式相減得到𝑾 ᵀ(Ẋ - Ẍ)＝0，我們可以發現(Ẋ - Ẍ)就是我們在超平面上的向量，而他跟𝑾內積為零更證明了𝑾是垂直於超平面的法向量(如圖9)。既然我們已經確定𝑾為超平面的法向量，那麼當我們在平面外有一個點𝑿到超平面任一點Ẋ的向量對𝑾做投影的距離，就是該點𝑿到超平面的距離。
<br />
p.s. 因為𝑾 ᵀẌ ＝ 𝒃
<br />
(補圖8，立體空間有一平面，上面有兩個點分別標注Ẋ及Ẍ，並且在兩點中間畫條向量的線)
(補圖9，立體空間有一平面，標注垂直平面的虛線W及平面外的點X，X點與平面上Ẋ連線並標註投影到W為X與該平面間距)

然後我們又希望我們的超平面可以將我們的點都分得正確，意思就是說output出來的值要跟預期的值同號，因此
𝒚n (𝑾 ᵀ𝑿n- 𝒃) > 0 所以我們就得到了我們要求的極值以及極值的限制條件。
要求MAX：Margin，而我們要求的Margin是要是最小的投影
限制條件：𝒚n (𝑾 ᵀ𝑿n- 𝒃) > 0

雖然已經知道我們要求的極值和限制條件，但是還是太難懂要怎麼求，所以我們再把他簡化，那...我們要從何處下手來簡化這個式子呢？不如就從我們的限制式下手吧！

因為我們向量投影公式是絕對值，所以我們為了把𝑾 ᵀ𝑿n- 𝒃永遠為正，因此乘上與之同號的𝒚n便可以將上面的部分看成𝒚n (𝑾 ᵀ𝑿n- 𝒃)，這樣的話我們就可以將我們要求的極值一下子簡化成求所有投影中的極小值：

上述限制是為什麼可以這樣放寬呢？其實在求極值時我們無論怎麼倍增或縮減長數項，對原本的極值是不會有改變的。例如：

各位一定有很多疑問？為什麼明明要求最大的Margin裡面卻又是變成求最小呢？答案很簡單，因為這個點在全部的點中，一定是要最靠近超平面的點才來求這個點離超平面最大的Margin，所以該點相較於其他點對超平面的距離要是最小的，所以我們才要用找最小值來求最近的點，然後又希望這個點離超平面的Margin越大越好。

對了～因為我們剛剛一直是用一邊的距離來推倒這樣的簡化式子，但是其實我們要求的Margin是兩個邊線最大的距離，所以我們要將投影的距離*2，這才是我們要求的正確Margin，因此我們的要求的最大的最小Margin就是：

這樣我們已經將要求的部分簡化了許多，但是因為是要求MAX的Margin，然而要求極大值是比較麻煩一點的...所以我們就想一下辦法改成求極小值吧！那我們應該要從哪邊下手呢...？我們可以分子分母倒過來！不就可以變成求極小值了嘛。再來因為我們W因為是範數(註2)，所以有根號比較難以做之後的推導，因此我們可以利用上面放寬公式的原理，去將根號去除掉，因此我們要求的問題就可以變成如下了：

其實算式到這邊就可以用典型的QP(quadratic programming)方式去解出來了，我們這邊就帶一個題目來給大家看看：我們有四個點X1=[0,0] X2=[2,2] X3=[2,0] X4=[3,0] ，y分別是-1 , -1, 1, 1，我們未知的W=[w1,w2]，
將四點帶入我們上面的限制式可以知道：

透過排列組合的加加減減後，可以得知我們的w1=1、w2=-1、b=1。

大家一定覺得說～這樣就可以解了！為什麼後面還有一堆東西要講解呢？那是因為這個QP方法實在是效率太低了，可以試想當我們的資料有幾萬筆、每筆的特徵又有幾千個，這樣的做法是非常非常消耗效能的。

經由上面的範例可以知道，雖然可以用QP方法解，但是當數據一複雜或是多的時候，就會變得很難去計算他並且效率也不好，因此我們需要用其他方式去加快他的速度及效能。
既然我們要用其他方式去解，我們可以發現我們要求的是極值問題，並且有限制式的存在，因此可以想到用Lagrange Multiplier加入Lagrange  𝒂的方式來計算：

再利用對Lagrange Multiplier做偏倒數得到下列兩個式子：


再來我們就來簡化我們原本的Lagrange Multiplier的式子，因為我們原本的式子裡面有未知數𝑾還有𝒃，所以我們要想辦法把他們都替換掉，以便之後求解用，首先我們將原式子的𝜮那項展開：

展開後我們發現𝒃的𝜮𝒂𝒚項是等於0的，因此我們可以把他砍了：

然後我們知道𝑾等於𝜮𝒂𝒚𝑥，所以我們把第一項的一個𝑾代掉，並且把第二項的𝑾一到常數項：

然後我們可以發現！第一項跟第二項是可以相減的，因此就不用客氣的直接相減：

接著我們把最後的𝑾也代掉，但因為是相乘，所以為了區別我們用j來表示：

在這邊我們將第一項的𝜮乘入變成一個𝜮，是利用(a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+…的乘法運算法：

最後我們再稍微整理一下，將正的部分擺在前面就完成了：

到這邊我們已經將Lagrange Multiplier中我們的未知數都替換成Lagrange因子了，所以接下來我們只要對Lagrange因子去做計算，就可以利用Lagrange Multiplier的特性求出我們的𝑾，透過知道𝑾我們就可以知道𝒃了，至於詳細要怎麼去做這些事情，我們後面會慢慢地講解。

註1：我們用大寫一般代表為一個矩陣，小寫的話為矩陣內的值
註2：範數也稱作歐幾里德距離，是我們常見的距離公式。
